{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reverse-cathedral",
   "metadata": {},
   "source": [
    "# HW2 - Transfer Learning with ResNet on STL-10 Dataset\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "This notebook was used in the 50.039 Deep Learning course at the Singapore University of Technology and Design.\n",
    "\n",
    "**Author:** Matthieu DE MARI (matthieu_demari@sutd.edu.sg)\n",
    "\n",
    "**Version:** 1.0 (27/02/2025)\n",
    "\n",
    "**Requirements:**\n",
    "- Python 3 (tested on v3.9.6)\n",
    "- Matplotlib (tested on v3.9.4)\n",
    "- Numpy (tested on v1.22.1)\n",
    "- Torch (tested on v1.12.1)\n",
    "- torchvision (tested on v0.13.1)\n",
    "\n",
    "**Deliverables**\n",
    "\n",
    "This HW notebook is less guided than HW1, on purpose. We expect your code/answers to be assembled in a small PDF report, discussing your implementations, answers to the questions below, along with some discussions about your observations/conclusions. As an additional challenge, you may propose additional implementations that are able to achieve higher accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-failure",
   "metadata": {},
   "source": [
    "### 0. Prelim: Imports needed and testing for CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from resnet50 import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-hundred",
   "metadata": {},
   "source": [
    "We advise running on GPU and setting up CUDA on your machine as it might drastically speed up the running time for this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device for torch\n",
    "use_cuda = True\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-craft",
   "metadata": {},
   "source": [
    "### 1. Prelim: Dataset and Data Augmentation\n",
    "This assignment focuses on adapting a pre-trained ResNet50 model to a new image classification task using transfer learning. The process involves two main steps:\n",
    "- Pre-training: First, we will train ResNet50 on the CIFAR-10 dataset using resnet50.py.\n",
    "- Fine-tuning: Next, we will fine-tune the model using the STL-10 dataset to adapt it to a new domain.\n",
    "\n",
    "Both CIFAR-10 and STL-10 contain 10 classes, representing airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. However, they differ in image resolution:\n",
    "- CIFAR-10: 32×32 pixels\n",
    "- STL-10: 96×96 pixels\n",
    " \n",
    "**Question 1:** \n",
    "- Based on the description for the dataset, could you describe what is the machine learning task here (supervised/unsupervised, classification/regression)?\n",
    "- How many input features (e.g., image dimensions, channels) and output classes are present?\n",
    "- What is the purpose of the transforms.Resize((224, 224)) operation in the transform variable below? Why is it needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Preprocessing\"\"\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c74c1b",
   "metadata": {},
   "source": [
    "To maintain computational efficiency and reduce execution time, we will use a subset of the STL-10 dataset:\n",
    "- Training phase: 100 samples per class, resulting in 1,000 training images.\n",
    "- Testing phase: 50 samples per class, resulting in 500 test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# (This might take a while as the dataset are quite big)\n",
    "train_dataset = torchvision.datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, [i for i in range(0, 1000)])\n",
    "test_dataset = torchvision.datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, [i for i in range(0, 500)])\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-capitol",
   "metadata": {},
   "source": [
    "### 2. Prelim: Our pre-trained Model \n",
    "\n",
    "We will use a simple pre-trained resnet model, with architecture and trainer stored in *resnet50.py* and weights stored in file *resnet50.data*.\n",
    "This is a pre-trained model with a simple architecture and its baseline accuracy is 80.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained model\n",
    "pretrained_model_weight = \"./resnet50.data\"\n",
    "state_dict = torch.load(pretrained_model_weight, map_location=device)\n",
    "model = resnet50(num_classes=10)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-winner",
   "metadata": {},
   "source": [
    "### 3. Transfer Learning Strategies\n",
    "\n",
    "In this assignment, we explore two transfer learning strategies to adapt a pre-trained ResNet50 model for the STL-10 dataset:\n",
    "1. Freeze all layers of ResNet50 except the final linear layer.\n",
    "2. Fine-tune the entire pre-trained model.\n",
    "\n",
    "**Question 2:** How would you modify the architecture to work with the STL-10 Dataset with these two strategies? Write code to replace the missing part and show your code in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1: only train fc\n",
    "model1 = resnet50(num_classes=10)\n",
    "model1.load_state_dict(state_dict, strict=False)\n",
    "model1.to(device)\n",
    "for param in model1.parameters():\n",
    "    None\n",
    "for param in model1.fc.parameters():  \n",
    "    None\n",
    "optimizer1 = optim.SGD(None, lr=0.005, momentum=0.9)\n",
    "\n",
    "# model2: fine-tune the entire model\n",
    "model2 = resnet50(num_classes=10)\n",
    "model2.load_state_dict(state_dict, strict=False)\n",
    "model2.to(device)\n",
    "optimizer2 = optim.SGD(None, lr=0.01, momentum=0.9)\n",
    "for param in model2.parameters():\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-providence",
   "metadata": {},
   "source": [
    "**Question 3:** Next, we will use two transfer learning strategies to retrain and test the models on the new dataset. Fill in the missing parts and show your code in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3648704f6dad862",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(None)\n",
    "test_loader = DataLoader(None)\n",
    "\n",
    "# loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# log\n",
    "train_losses = {}\n",
    "val_losses = {}\n",
    "val_accuracies = {}\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, optimizer, num_epochs):\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = None\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = None\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = None\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = None\n",
    "                outputs = model(images)\n",
    "                loss = None\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = None\n",
    "        accuracy = None\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        val_acc_list.append(accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {accuracy:.4%}\")\n",
    "\n",
    "    return train_loss_list, val_loss_list, val_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439dc715a3bcfb99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4. Visualization and Analysis\n",
    "**Question 4:** Plot training/validation loss curves for both strategies.\n",
    "- Compare their final test accuracies. Which strategy is more effective? Why or why not?\n",
    "- Does unfreezing more layers always improve performance? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "results = {}\n",
    "print(f\"\\nTraining Strategy 1\")\n",
    "results[f\"Strategy1\"] = train_and_evaluate(model1, train_loader, test_loader, optimizer1, num_epochs)\n",
    "print(f\"\\nTraining Strategy 2 \")\n",
    "results[f\"Strategy2\"] = train_and_evaluate(model2, train_loader, test_loader, optimizer2, num_epochs)\n",
    "\n",
    "# print\n",
    "for key, (train_loss, val_loss, val_acc) in results.items():\n",
    "    print(f\"\\nResults for {key}:\")\n",
    "    print(f\"  Train Loss: {train_loss}\")\n",
    "    print(f\"  Val Loss: {val_loss}\")\n",
    "    print(f\"  Val Accuracy: {val_acc}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key, (train_loss, val_loss, val_acc) in results.items():\n",
    "    plt.plot(train_loss, label=f'Train Loss ({key})', linestyle=\"--\")\n",
    "    plt.plot(val_loss, label=f'Val Loss ({key})')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss of Different Learning Strategies\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key, (train_loss, val_loss, val_acc) in results.items():\n",
    "    plt.plot(val_acc, label=f'Val Acc ({key})')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Validation Accuracy of Different Learning Strategies\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1475dc95b0da03c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5. Data Augmentation\n",
    "Next, we will investigate the effectiveness of data augmentation in improving model performance. To do this, we implement and compare two pre-training strategies using the CIFAR-10 dataset:\n",
    "\n",
    "Without Data Augmentation: Train the model on CIFAR-10 without applying any augmentation techniques (as implemented in resnet50.py).\n",
    "With Data Augmentation: Incorporate data augmentation techniques to train the model.\n",
    "\n",
    "**Question 5:** For the second strategy, apply the following transformations to augment the dataset and complete the missing parts in the code:\n",
    "- Random horizontal flipping with a probability of 0.5\n",
    "- Random rotation up to 15 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f2934a3dc6070",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Data Augmentation and Preprocessing\"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    None,  # Flipping probability = 0.5\n",
    "    None,   # Rotation ±15 degrees\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea741f1caa5d0b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Using the aforementioned data augmentation techniques, we first pre-train the model on the CIFAR-10 dataset and save the model parameters as resnet_aug.data.\n",
    "Next, we fine-tune the model on the STL-10 dataset and compare the results of the two training strategies (with and without data augmentation). \n",
    "\n",
    "**Question 6:** Explain why data augmentation is critical for small datasets. How might these transforms improve model generalization? Are the transformations we suggested in this notebook appropriate? Would you suggest using different ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50746b320e26cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune with a very small dataset\n",
    "train_dataset_small = torchvision.datasets.STL10(root='./data', split='train', download=True, transform=transform)\n",
    "train_dataset_small = torch.utils.data.Subset(train_dataset_small, [i for i in range(0, 300)])\n",
    "train_loader_small = DataLoader(train_dataset_small, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# model3: without data augmentation\n",
    "model3 = resnet50(num_classes=10)\n",
    "model3.load_state_dict(torch.load('./resnet50.data', map_location=device), strict=False)\n",
    "model3.to(device)\n",
    "optimizer3 = optim.SGD(model3.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# model4: with data augmentation\n",
    "model4 = resnet50(num_classes=10)\n",
    "model4.load_state_dict(torch.load('./resnet50_aug.data', map_location=device), strict=False)\n",
    "model4.to(device)\n",
    "optimizer4 = optim.SGD(model4.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "results = {}\n",
    "print(f\"\\nTraining Strategy 2 without augmentation \")\n",
    "results[f\"Strategy2_no_aug\"] = train_and_evaluate(model3, train_loader_small, test_loader, optimizer3, 50)\n",
    "print(f\"\\nTraining Strategy 2 with augmentation \")\n",
    "results[f\"Strategy2_aug\"] = train_and_evaluate(model4, train_loader_small, test_loader, optimizer4, 50)\n",
    "\n",
    "# print\n",
    "for key, (train_loss, val_loss, val_acc) in results.items():\n",
    "    print(f\"\\nResults for {key}:\")\n",
    "    print(f\"  Train Loss: {train_loss}\")\n",
    "    print(f\"  Val Loss: {val_loss}\")\n",
    "    print(f\"  Val Accuracy: {val_acc}\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for key, (train_loss, val_loss, val_acc) in results.items():\n",
    "    plt.plot(val_acc, label=f'Val Acc ({key})')\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699769fb-4bdb-46ad-8162-b3672e7255cc",
   "metadata": {},
   "source": [
    "**Question 7:** Can you suggest another way to perform transfer learning on these models that might be more effective than the current implementation? What would you do differently? Extra points will be given based on effort and performance of the proposed approach. You might want to consider dropping the transforms.Resize((224, 224)) operation from earlier and adjust/retrain some layers from the ResNet architecture instead as shown in class on W4S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d4e377-b800-47f6-8930-56d297602993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
